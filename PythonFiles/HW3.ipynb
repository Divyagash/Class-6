{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Neuralnets(object):\n",
    "    \n",
    "    def __init__(self,classes):\n",
    "        self.classes = classes\n",
    "        \n",
    "    \n",
    "    # Initialize the weights as a numpy array (All weights will be matrices)\n",
    "    # Initialize the bias terms as a numpy array as well (Bias terms will be vectors)\n",
    "    \n",
    "    def buildNeuralNetworks(self,X,y,weightStorage,biasStorage,hL,nodes,iteration,eta):\n",
    "        # SGD\n",
    "        \n",
    "        newWeight = weightStorage\n",
    "        newBias = biasStorage\n",
    "        WeightList = []\n",
    "        biasList = []\n",
    "        for i in xrange(0,iteration):\n",
    "            #np.random.shuffle(X)\n",
    "            samples,features = X.shape\n",
    "            for j in xrange(0,samples):\n",
    "                #print j\n",
    "                #print '---'\n",
    "                x = X[j,]\n",
    "            \n",
    "                # Feed Forward\n",
    "                # Store W,a,f'(z)\n",
    "                #weightStorage = np.empty()\n",
    "                valueStorage = []\n",
    "                activeDerivStorage = []\n",
    "                #a = x\n",
    "                valueStorage.append(x)\n",
    "                #print x\n",
    "                for k in range(hL+1):\n",
    "                    \n",
    "                    z =np.dot(np.transpose(newWeight[k]),valueStorage[k]) +newBias[k]\n",
    "                    #print '--z-value--'\n",
    "                    #print z\n",
    "                    if k != hL:\n",
    "                        #print z\n",
    "                        f = np.vectorize(self.ReLU)\n",
    "                        a = f(z)\n",
    "                        \n",
    "   \n",
    "                        \n",
    "                        valueStorage.append(a)\n",
    "                        #print valueStorage.shape\n",
    "                        fPrime = np.vectorize(self.ReLUDeriv)\n",
    "                        activeDerivStorage.append(fPrime(z))\n",
    "                    else:\n",
    "                        #f = np.vectorize(self.softmax)\n",
    "                        a = self.softmax(z)\n",
    "                        \n",
    "                        valueStorage.append(a)\n",
    "                        #fPrime = np.vectorize(self.softmaxDeriv)\n",
    "                        activeDerivStorage.append(self.softmaxDeriv(z))\n",
    "\n",
    "                outputError =  valueStorage[hL+1] - y[j]\n",
    "                \n",
    "                deltaStorage = [None]*(hL+1)\n",
    "                deltaStorage[hL] = outputError\n",
    "                for l in reversed(range(hL)):\n",
    "\n",
    "                    delta = np.dot(np.dot(np.diag(activeDerivStorage[l]),newWeight[l+1]),deltaStorage[l+1])\n",
    "                    deltaStorage[l] = np.array(delta)\n",
    "                    #print deltaStorage\n",
    "                 \n",
    "                updatedWeights = []\n",
    "                updatedBias = []\n",
    "                for k in range(0,hL+1):\n",
    "\n",
    "                    weightGradient = np.outer(valueStorage[k],deltaStorage[k])\n",
    "                    #print weightGradient\n",
    "                    biasGradient = deltaStorage[k]\n",
    "                    newWeight[k] = newWeight[k] - (eta*weightGradient)\n",
    "                    updatedWeights.append(newWeight[k])\n",
    "                    #print '--weight Matrices--'\n",
    "                    #print newWeight[k]\n",
    "                    newBias[k] = newBias[k] - (eta*biasGradient)\n",
    "                    updatedBias.append(newBias[k])\n",
    "\n",
    "                WeightList.append(updatedWeights) \n",
    "                biasList.append(updatedBias)  \n",
    "                #print newWeight\n",
    "        return (WeightList[-1],biasList[-1])\n",
    "                    \n",
    "    \n",
    "    \n",
    "    def predictNN(self,X,weight,bias):\n",
    "\n",
    "        predictions = []\n",
    "        for j in range(X.shape[0]):\n",
    "            a = [np.transpose(np.array([X[j,:]]))]\n",
    "            #print a\n",
    "            #y = np.array([Y[j]])\n",
    "            z = []\n",
    "            \n",
    "            for k in range(len(weight)):\n",
    "                \n",
    "                z.append(np.array(np.dot(np.transpose(weight[k]), a[k])+np.transpose(np.array([bias[k]]))))\n",
    "                if k < (len(weight)-1):\n",
    "                    #print z[k]\n",
    "                    f = np.vectorize(self.ReLU)\n",
    "                    #print f(z[k])\n",
    "                    a.append(np.array(f(z[k])))\n",
    "                    #print z[k]\n",
    "                else:\n",
    "                    #print z[k]\n",
    "                    a.append(np.array(self.softmax(z[k])))\n",
    "\n",
    "            max_prob = np.max(a[len(weight)])\n",
    "            #print max_prob\n",
    "            #print list(a[len(weight)])\n",
    "            max_index = list(a[len(weight)]).index(max_prob)\n",
    "            prediction = np.zeros(len(a[len(weight)]))\n",
    "            prediction[max_index] = 1\n",
    "            predictions.append(prediction)\n",
    "        #classification_accuracy = sum([all(predictions[i]==array(Y)[i]) for i in range(len(Y))])/len(Y)\n",
    "        return(predictions)\n",
    "        \n",
    "    def ClassificationError(self,pred,Y):\n",
    "        \"\"\" Provides the misclassification error given the true value and the prediction\"\"\"\n",
    "        \n",
    "        cError = 1 - sum([all(pred[i]==Y[i]) for i in range(len(Y))])/np.float(len(Y))\n",
    "        return(cError)\n",
    "                \n",
    "    def ParameterInitialization(self,X,y,nodes):\n",
    "        # Control the number of nodes in the weight initialization function\n",
    "        # nodes variable should be a list of number of nodes in each hidden layer\n",
    "        weightList = []\n",
    "        biasList = []\n",
    "        allNodes = [X.shape[1]]+nodes+[y.shape[1]]\n",
    "        \n",
    "        for i in range(1,len(allNodes)):\n",
    "            weight = np.empty([allNodes[i-1],allNodes[i]])\n",
    "            # Gaussian random weights with variance 1/sqrt(m)\n",
    "            weight = np.reshape(np.random.normal(0, 1/np.sqrt(allNodes[i-1]),weight.size), (allNodes[i-1],allNodes[i]))\n",
    "            weightList.append(weight)\n",
    "            \n",
    "            bias = np.random.normal(0,1/np.sqrt((allNodes[i])),allNodes[i])\n",
    "            biasList.append(bias)\n",
    "        return(weightList,biasList)\n",
    "            \n",
    "    \n",
    "    \n",
    "    def ReLU(self,z):\n",
    "        return np.amax([0,z])\n",
    "    \n",
    "    def softmax(self,Z):\n",
    "            #denom = sum(np.exp(Z))\n",
    "        #return(np.exp(Z)/denom)\n",
    "        e_x = np.exp(Z - np.max(Z))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    \n",
    "    def ReLUDeriv(self,z):\n",
    "        if z>0:\n",
    "            Derive = 1\n",
    "        else:\n",
    "            # subgradient at zero was set to zero\n",
    "            Derive = 0\n",
    "        return(Derive)\n",
    "    \n",
    "    def softmaxDeriv(self,Z):\n",
    "        n = len(Z)\n",
    "        denom = sum(np.exp(Z))\n",
    "        \n",
    "        Jacobian = np.empty([n,n])\n",
    "        for i in range(n):\n",
    "            for j in range(i+1):\n",
    "                if i == j:\n",
    "                    Jacobian[i,j] = (np.exp(Z[i])/denom) - (np.exp(Z[i])/denom)**2\n",
    "                    Jacobian[j,i] = (np.exp(Z[i])/denom) - (np.exp(Z[i])/denom)**2\n",
    "                else:\n",
    "                    Jacobian[i,j] = (np.exp(-(Z[i]+Z[j]))/denom)**2\n",
    "                    Jacobian[j,i] = (np.exp(-(Z[i]+Z[j]))/denom)**2\n",
    "        return (Jacobian)\n",
    "                \n",
    "    def crossEntropyLoss(self, y,guess):\n",
    "        return(-(y/guess))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the given data_3class.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pl.loadtxt('../DataFiles/HW3/data_3class.csv')\n",
    "\n",
    "\n",
    "X = train[:, 0:2]\n",
    "Y = train[:, 2:3]\n",
    "Y\n",
    "len(Y)\n",
    "Y_encode = np.int_(np.reshape(Y,(800,)))\n",
    "b = np.zeros((len(Y_encode), 3))\n",
    "b[np.arange(len(Y_encode)), Y_encode] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sub = X[0:49,]\n",
    "b_sub = b[0:49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NNexercise_data3 = Neuralnets(3)\n",
    "nodes = [3]\n",
    "NNexercise_data3.softmax(np.array([[2.3],[3.4]]))\n",
    "Weights,bias = NNexercise_data3.ParameterInitialization(X_sub,b_sub,nodes)\n",
    "#print Weights,bias\n",
    "#Weights2,bias2 = NNexercise_data3.buildNeuralNetworks(X_sub,b_sub,Weights,bias,1,nodes,1,1)\n",
    "W,B = NNexercise_data3.buildNeuralNetworks(X_sub,b_sub,Weights,bias,1,nodes,500,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = NNexercise_data3.predictNN(X_sub,W,B)\n",
    "#print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4897959183673469"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNexercise_data3.ClassificationError(pred,b_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Weights,bias = NNexercise_data3.ParameterInitialization(X,b,nodes)\n",
    "Weights2,bias2 = NNexercise_data3.ParameterInitialization(X,b,nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = NNexercise_data3.predictNN(X,Weights,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NNexercise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-321590403066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNexercise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWeights2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'NNexercise' is not defined"
     ]
    }
   ],
   "source": [
    "pred2 = NNexercise.predictNN(X,Weights2,bias2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66875"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNexercise.ClassificationError(pred2,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "buildNeuralNetworks() takes exactly 9 arguments (7 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-207cab9b8dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Params = NNexercise_data3.ParameterInitialization(X,b,nodes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print Params[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mWeight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNexercise_data3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildNeuralNetworks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#print bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: buildNeuralNetworks() takes exactly 9 arguments (7 given)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "NNexercise_data3 = Neuralnets(3)\n",
    "nodes = [3]\n",
    "#Params = NNexercise_data3.ParameterInitialization(X,b,nodes)\n",
    "#print Params[0]\n",
    "Weight,bias = NNexercise_data3.buildNeuralNetworks(X,b,1,nodes,100,0.001)\n",
    "#print bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.59000996, -1.17710391,  1.18395388],\n",
       "        [-1.2303208 , -1.19403115, -0.70056632]]),\n",
       " array([[ 0.01547935, -0.04329687,  0.06930713],\n",
       "        [ 0.03615068,  0.21948215,  0.01819244],\n",
       "        [ 0.02567221,  0.18030655,  0.19062912]])]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NNexercise.predictNN(X,Weight,bias)\n",
    "pred = NNexercise.predictNN(X_sub,Weight,bias)\n",
    "#pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNexercise.ClassificationError(pred,b_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#b_sub\\\n",
    "pred = NNexercise.predictNN(X_sub,Weight,bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNexercise.ClassificationError(pred,b_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorized functions\n",
    "\n",
    "def ReLU(z):\n",
    "    return(np.amax([0,z]))\n",
    "\n",
    "vReLU = np.vectorize(ReLU)\n",
    "\n",
    "def softmax(z):\n",
    "    z = np.array(z)-np.max(z)\n",
    "    return(np.exp(z)/sum(np.exp(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One small  hidden layer - 3 \n",
    "#One large hidden layer - 50 \n",
    "#Two small hidden layer - [3,4] \n",
    "#Two large hidden layer - [50,60] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2D Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset 1\n",
    "\n",
    "data = pl.loadtxt('../DataFiles/HW2/data1_train.csv')\n",
    "len(data[:,2])\n",
    "Y = data[:,2]\n",
    "Y = Y *0.5+0.5\n",
    "Y = np.int_(np.reshape(Y,(len(Y,))))\n",
    "b = np.zeros((len(Y), 2))\n",
    "b[np.arange(len(Y)), Y] = 1\n",
    "b\n",
    "Y1_Train = b\n",
    "X1_Train = data[:,0:2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data1_V = pl.loadtxt('../DataFiles/HW2/data1_validate.csv')\n",
    "Y1_V = data1_V[:,2]\n",
    "Y1_V = Y1_V *0.5+0.5\n",
    "Y1_V = np.int_(np.reshape(Y1_V,(len(Y1_V,))))\n",
    "b = np.zeros((len(Y1_V), 2))\n",
    "b[np.arange(len(Y1_V)), Y1_V] = 1\n",
    "b\n",
    "Y1_V = b\n",
    "X1_V = data1_V[:,0:2]\n",
    "\n",
    "\n",
    "\n",
    "data1_Test = pl.loadtxt('../DataFiles/HW2/data1_test.csv')\n",
    "Y1_Test = data1_Test[:,2]\n",
    "Y1_Test = Y1_Test *0.5+0.5\n",
    "Y1_Test = np.int_(np.reshape(Y1_Test,(len(Y1_Test,))))\n",
    "b = np.zeros((len(Y1_Test), 2))\n",
    "b[np.arange(len(Y1_Test)), Y1_Test] = 1\n",
    "b\n",
    "Y1_Test = b\n",
    "X1_Test = data1_Test[:,0:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset 2\n",
    "\n",
    "data = pl.loadtxt('../DataFiles/HW2/data2_train.csv')\n",
    "len(data[:,2])\n",
    "Y2_Train = data[:,2]\n",
    "Y2_Train = Y2_Train *0.5+0.5\n",
    "Y2_Train = np.int_(np.reshape(Y2_Train,(len(Y2_Train,))))\n",
    "b = np.zeros((len(Y2_Train), 2))\n",
    "b[np.arange(len(Y2_Train)), Y2_Train] = 1\n",
    "b\n",
    "Y2_Train = b\n",
    "X2_Train = data[:,0:2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_V = pl.loadtxt('../DataFiles/HW2/data2_validate.csv')\n",
    "Y2_V = data_V[:,2]\n",
    "Y2_V = Y2_V *0.5+0.5\n",
    "Y2_V = np.int_(np.reshape(Y2_V,(len(Y2_V,))))\n",
    "b = np.zeros((len(Y2_V), 2))\n",
    "b[np.arange(len(Y2_V)), Y2_V] = 1\n",
    "b\n",
    "Y2_V = b\n",
    "X2_V = data_V[:,0:2]\n",
    "\n",
    "\n",
    "\n",
    "data_Test = pl.loadtxt('../DataFiles/HW2/data1_test.csv')\n",
    "Y2_Test = data_Test[:,2]\n",
    "Y2_Test = Y2_Test *0.5+0.5\n",
    "Y2_Test = np.int_(np.reshape(Y2_Test,(len(Y2_Test,))))\n",
    "b = np.zeros((len(Y2_Test), 2))\n",
    "b[np.arange(len(Y2_Test)), Y2_Test] = 1\n",
    "b\n",
    "Y2_Test = b\n",
    "X2_Test = data_Test[:,0:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset 3\n",
    "\n",
    "data = pl.loadtxt('../DataFiles/HW2/data3_train.csv')\n",
    "len(data[:,2])\n",
    "Y3_Train = data[:,2]\n",
    "Y3_Train = Y3_Train *0.5+0.5\n",
    "Y3_Train = np.int_(np.reshape(Y3_Train,(len(Y3_Train,))))\n",
    "b = np.zeros((len(Y3_Train), 2))\n",
    "b[np.arange(len(Y3_Train)), Y3_Train] = 1\n",
    "b\n",
    "Y3_Train = b\n",
    "X3_Train = data[:,0:2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_V = pl.loadtxt('../DataFiles/HW2/data3_validate.csv')\n",
    "Y3_V = data_V[:,2]\n",
    "Y3_V = Y3_V *0.5+0.5\n",
    "Y3_V = np.int_(np.reshape(Y3_V,(len(Y3_V,))))\n",
    "b = np.zeros((len(Y3_V), 2))\n",
    "b[np.arange(len(Y3_V)), Y3_V] = 1\n",
    "b\n",
    "Y3_V = b\n",
    "X3_V = data_V[:,0:2]\n",
    "\n",
    "\n",
    "\n",
    "data_Test = pl.loadtxt('../DataFiles/HW2/data3_test.csv')\n",
    "Y3_Test = data_Test[:,2]\n",
    "Y3_Test = Y3_Test *0.5+0.5\n",
    "Y3_Test = np.int_(np.reshape(Y3_Test,(len(Y3_Test,))))\n",
    "b = np.zeros((len(Y3_Test), 2))\n",
    "b[np.arange(len(Y3_Test)), Y3_Test] = 1\n",
    "b\n",
    "Y3_Test = b\n",
    "X3_Test = data_Test[:,0:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset 4\n",
    "\n",
    "data = pl.loadtxt('../DataFiles/HW2/data4_train.csv')\n",
    "len(data[:,2])\n",
    "Y4_Train = data[:,2]\n",
    "Y4_Train = Y4_Train *0.5+0.5\n",
    "Y4_Train = np.int_(np.reshape(Y4_Train,(len(Y4_Train,))))\n",
    "b = np.zeros((len(Y4_Train), 2))\n",
    "b[np.arange(len(Y4_Train)), Y4_Train] = 1\n",
    "b\n",
    "Y4_Train = b\n",
    "X4_Train = data[:,0:2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_V = pl.loadtxt('../DataFiles/HW2/data4_validate.csv')\n",
    "Y4_V = data_V[:,2]\n",
    "Y4_V = Y4_V *0.5+0.5\n",
    "Y4_V = np.int_(np.reshape(Y4_V,(len(Y4_V,))))\n",
    "b = np.zeros((len(Y4_V), 2))\n",
    "b[np.arange(len(Y4_V)), Y4_V] = 1\n",
    "b\n",
    "Y4_V = b\n",
    "X4_V = data_V[:,0:2]\n",
    "\n",
    "\n",
    "\n",
    "data_Test = pl.loadtxt('../DataFiles/HW2/data4_test.csv')\n",
    "Y4_Test = data_Test[:,2]\n",
    "Y4_Test = Y4_Test *0.5+0.5\n",
    "Y4_Test = np.int_(np.reshape(Y4_Test,(len(Y4_Test,))))\n",
    "b = np.zeros((len(Y4_Test), 2))\n",
    "b[np.arange(len(Y4_Test)), Y4_Test] = 1\n",
    "b\n",
    "Y4_Test = b\n",
    "X4_Test = data_Test[:,0:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5025"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One small hidden layer \n",
    "NNexercise_data3 = Neuralnets(3)\n",
    "nodes = [3]\n",
    "Weight,bias = NNexercise_data3.buildNeuralNetworks(X1_Train,Y1_Train,1,nodes,500,0.0001)\n",
    "pred = NNexercise.predictNN(X1_Train,Weight,bias)\n",
    "NNexercise.ClassificationError(pred,Y1_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = NNexercise.predictNN(X1_Test,Weight,bias)\n",
    "NNexercise.ClassificationError(pred,Y1_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5125"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One large hidden layer \n",
    "NNexercise_data3 = Neuralnets(3)\n",
    "nodes = [50]\n",
    "Weight,bias = NNexercise_data3.buildNeuralNetworks(X1_Train,Y1_Train,1,nodes,100,0.0001)\n",
    "pred = NNexercise.predictNN(X1_Train,Weight,bias)\n",
    "NNexercise.ClassificationError(pred,Y1_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "# 10 class digit classification\n",
    "# Atleast 200 training samples, validation and testing to be 150 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating Train, Test and Validation set for this question\n",
    "\n",
    "MNISTData_0 = pl.loadtxt('../DataFiles/HW2/mnist_digit_0.csv')\n",
    "MNISTData_0_Train =MNISTData_0[:200,]\n",
    "MNISTData_0_Valid =MNISTData_0[201:351,]\n",
    "MNISTData_0_Test =MNISTData_0[351:501,]\n",
    "\n",
    "MNISTData_1 = pl.loadtxt('../DataFiles/HW2/mnist_digit_1.csv')\n",
    "MNISTData_1_Train =MNISTData_1[:200,]\n",
    "MNISTData_1_Valid =MNISTData_1[201:351,]\n",
    "MNISTData_1_Test =MNISTData_1[351:501,]\n",
    "\n",
    "MNISTData_2 = pl.loadtxt('../DataFiles/HW2/mnist_digit_2.csv')\n",
    "MNISTData_2_Train =MNISTData_2[:200,]\n",
    "MNISTData_2_Valid =MNISTData_2[201:351,]\n",
    "MNISTData_2_Test =MNISTData_2[351:501,]\n",
    "\n",
    "MNISTData_3 = pl.loadtxt('../DataFiles/HW2/mnist_digit_3.csv')\n",
    "MNISTData_3_Train =MNISTData_3[:200,]\n",
    "MNISTData_3_Valid =MNISTData_3[201:351,]\n",
    "MNISTData_3_Test =MNISTData_3[351:501,]\n",
    "\n",
    "MNISTData_4 = pl.loadtxt('../DataFiles/HW2/mnist_digit_4.csv')\n",
    "MNISTData_4_Train =MNISTData_4[:200,]\n",
    "MNISTData_4_Valid =MNISTData_4[201:351,]\n",
    "MNISTData_4_Test =MNISTData_4[351:501,]\n",
    "\n",
    "MNISTData_5 = pl.loadtxt('../DataFiles/HW2/mnist_digit_5.csv')\n",
    "MNISTData_5_Train =MNISTData_5[:200,]\n",
    "MNISTData_5_Valid =MNISTData_5[201:351,]\n",
    "MNISTData_5_Test =MNISTData_5[351:501,]\n",
    "\n",
    "MNISTData_6 = pl.loadtxt('../DataFiles/HW2/mnist_digit_6.csv')\n",
    "MNISTData_6_Train =MNISTData_6[:200,]\n",
    "MNISTData_6_Valid =MNISTData_6[201:351,]\n",
    "MNISTData_6_Test =MNISTData_6[351:501,]\n",
    "\n",
    "MNISTData_7 = pl.loadtxt('../DataFiles/HW2/mnist_digit_7.csv')\n",
    "MNISTData_7_Train =MNISTData_7[:200,]\n",
    "MNISTData_7_Valid =MNISTData_7[201:351,]\n",
    "MNISTData_7_Test =MNISTData_7[351:501,]\n",
    "\n",
    "MNISTData_8 = pl.loadtxt('../DataFiles/HW2/mnist_digit_8.csv')\n",
    "MNISTData_8_Train =MNISTData_8[:200,]\n",
    "MNISTData_8_Valid =MNISTData_8[201:351,]\n",
    "MNISTData_8_Test =MNISTData_8[351:501,]\n",
    "\n",
    "MNISTData_9 = pl.loadtxt('../DataFiles/HW2/mnist_digit_9.csv')\n",
    "MNISTData_9_Train =MNISTData_9[:200,]\n",
    "MNISTData_9_Valid =MNISTData_9[201:351,]\n",
    "MNISTData_9_Test =MNISTData_9[351:501,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MNISTData_0_Train_Augmented = np.c_[np.array([0]*MNISTData_0_Train.shape[0]), MNISTData_0_Train]\n",
    "MNISTData_0_Valid_Augmented = np.c_[np.array([0]*MNISTData_0_Valid.shape[0]), MNISTData_0_Valid]\n",
    "MNISTData_0_Test_Augmented = np.c_[np.array([0]*MNISTData_0_Test.shape[0]), MNISTData_0_Test]\n",
    "\n",
    "MNISTData_1_Train_Augmented = np.c_[np.array([1]*MNISTData_1_Train.shape[0]), MNISTData_1_Train]\n",
    "MNISTData_1_Valid_Augmented = np.c_[np.array([1]*MNISTData_1_Valid.shape[0]), MNISTData_1_Valid]\n",
    "MNISTData_1_Test_Augmented = np.c_[np.array([1]*MNISTData_1_Test.shape[0]), MNISTData_1_Test]\n",
    "\n",
    "MNISTData_2_Train_Augmented = np.c_[np.array([2]*MNISTData_2_Train.shape[0]), MNISTData_2_Train]\n",
    "MNISTData_2_Valid_Augmented = np.c_[np.array([2]*MNISTData_2_Valid.shape[0]), MNISTData_2_Valid]\n",
    "MNISTData_2_Test_Augmented = np.c_[np.array([2]*MNISTData_2_Test.shape[0]), MNISTData_2_Test]\n",
    "\n",
    "MNISTData_3_Train_Augmented = np.c_[np.array([3]*MNISTData_3_Train.shape[0]), MNISTData_3_Train]\n",
    "MNISTData_3_Valid_Augmented = np.c_[np.array([3]*MNISTData_3_Valid.shape[0]), MNISTData_3_Valid]\n",
    "MNISTData_3_Test_Augmented = np.c_[np.array([3]*MNISTData_3_Test.shape[0]), MNISTData_3_Test]\n",
    "\n",
    "MNISTData_4_Train_Augmented = np.c_[np.array([4]*MNISTData_4_Train.shape[0]), MNISTData_4_Train]\n",
    "MNISTData_4_Valid_Augmented = np.c_[np.array([4]*MNISTData_4_Valid.shape[0]), MNISTData_4_Valid]\n",
    "MNISTData_4_Test_Augmented = np.c_[np.array([4]*MNISTData_4_Test.shape[0]), MNISTData_4_Test]\n",
    "\n",
    "MNISTData_5_Train_Augmented = np.c_[np.array([5]*MNISTData_5_Train.shape[0]), MNISTData_5_Train]\n",
    "MNISTData_5_Valid_Augmented = np.c_[np.array([5]*MNISTData_5_Valid.shape[0]), MNISTData_5_Valid]\n",
    "MNISTData_5_Test_Augmented = np.c_[np.array([5]*MNISTData_5_Test.shape[0]), MNISTData_5_Test]\n",
    "\n",
    "MNISTData_6_Train_Augmented = np.c_[np.array([6]*MNISTData_6_Train.shape[0]), MNISTData_6_Train]\n",
    "MNISTData_6_Valid_Augmented = np.c_[np.array([6]*MNISTData_6_Valid.shape[0]), MNISTData_6_Valid]\n",
    "MNISTData_6_Test_Augmented = np.c_[np.array([6]*MNISTData_6_Test.shape[0]), MNISTData_6_Test]\n",
    "\n",
    "\n",
    "MNISTData_7_Train_Augmented = np.c_[np.array([7]*MNISTData_7_Train.shape[0]), MNISTData_7_Train]\n",
    "MNISTData_7_Valid_Augmented = np.c_[np.array([7]*MNISTData_7_Valid.shape[0]), MNISTData_7_Valid]\n",
    "MNISTData_7_Test_Augmented = np.c_[np.array([7]*MNISTData_7_Test.shape[0]), MNISTData_7_Test]\n",
    "\n",
    "MNISTData_8_Train_Augmented = np.c_[np.array([8]*MNISTData_8_Train.shape[0]), MNISTData_8_Train]\n",
    "MNISTData_8_Valid_Augmented = np.c_[np.array([8]*MNISTData_8_Valid.shape[0]), MNISTData_8_Valid]\n",
    "MNISTData_8_Test_Augmented = np.c_[np.array([8]*MNISTData_8_Test.shape[0]), MNISTData_8_Test]\n",
    "\n",
    "MNISTData_9_Train_Augmented = np.c_[np.array([9]*MNISTData_9_Train.shape[0]), MNISTData_9_Train]\n",
    "MNISTData_9_Valid_Augmented = np.c_[np.array([9]*MNISTData_9_Valid.shape[0]), MNISTData_9_Valid]\n",
    "MNISTData_9_Test_Augmented = np.c_[np.array([9]*MNISTData_9_Test.shape[0]), MNISTData_9_Test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Set_train = np.vstack([MNISTData_1_Train_Augmented,MNISTData_2_Train_Augmented,MNISTData_3_Train_Augmented,MNISTData_4_Train_Augmented,\n",
    "                      MNISTData_5_Train_Augmented,MNISTData_6_Train_Augmented,MNISTData_7_Train_Augmented,MNISTData_8_Train_Augmented,\n",
    "                      MNISTData_9_Train_Augmented,MNISTData_0_Train_Augmented])\n",
    "                      \n",
    "                      \n",
    "np.random.shuffle(Set_train)\n",
    "Set_train_X = Set_train[:,1:]\n",
    "Set_train_X_Norm = (2*Set_train_X/255)-1\n",
    "Set_train_Y = Set_train[:,0]\n",
    "\n",
    "Set_train_Y = np.int_(np.reshape(Set_train_Y,(len(Set_train_Y,))))\n",
    "b = np.zeros((len(Set_train_Y), 10))\n",
    "b[np.arange(len(Set_train_Y)), Set_train_Y] = 1\n",
    "Set_train_Y = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Set_valid = np.vstack([MNISTData_1_Valid_Augmented,MNISTData_2_Valid_Augmented,MNISTData_3_Valid_Augmented,MNISTData_4_Valid_Augmented,\n",
    "                      MNISTData_5_Valid_Augmented,MNISTData_6_Valid_Augmented,MNISTData_7_Valid_Augmented,MNISTData_8_Valid_Augmented,\n",
    "                      MNISTData_9_Valid_Augmented,MNISTData_0_Valid_Augmented])\n",
    "                      \n",
    "                      \n",
    "np.random.shuffle(Set_valid)\n",
    "Set_valid_X = Set_valid[:,1:]\n",
    "Set_valid_X_Norm = (2*Set_valid_X/255)-1\n",
    "Set_valid_Y = Set_valid[:,0]\n",
    "\n",
    "Set_valid_Y = np.int_(np.reshape(Set_valid_Y,(len(Set_valid_Y,))))\n",
    "b = np.zeros((len(Set_valid_Y), 10))\n",
    "b[np.arange(len(Set_valid_Y)), Set_valid_Y] = 1\n",
    "Set_valid_Y = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Set_test = np.vstack([MNISTData_1_Test_Augmented,MNISTData_2_Test_Augmented,MNISTData_3_Test_Augmented,MNISTData_4_Test_Augmented,\n",
    "                      MNISTData_5_Test_Augmented,MNISTData_6_Test_Augmented,MNISTData_7_Test_Augmented,MNISTData_8_Test_Augmented,\n",
    "                      MNISTData_9_Test_Augmented,MNISTData_0_Test_Augmented])\n",
    "                      \n",
    "                      \n",
    "np.random.shuffle(Set_test)\n",
    "Set_test_X = Set_valid[:,1:]\n",
    "Set_test_X_Norm = (2*Set_test_X/255)-1\n",
    "Set_test_Y = Set_test[:,0]\n",
    "\n",
    "Set_test_Y = np.int_(np.reshape(Set_test_Y,(len(Set_test_Y,))))\n",
    "b = np.zeros((len(Set_test_Y), 10))\n",
    "b[np.arange(len(Set_test_Y)), Set_test_Y] = 1\n",
    "Set_test_Y = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Done in the tensorflow file via command line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
